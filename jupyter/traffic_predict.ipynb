{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd383dd",
   "metadata": {},
   "source": [
    "## How TomTom represents traffic (brief)\n",
    "\n",
    "TomTom aggregates probe data (floating car / GPS reports from connected vehicles and apps) and station/sensor data to estimate traffic on road segments. The typical fields you will see in their `flowSegmentData` style responses and our CSV/JSON exports are:\n",
    "- `currentSpeed`: estimated current speed on the sampled point/segment (km/h).\n",
    "- `freeFlowSpeed`: the typical uncongested speed for the segment (km/h).\n",
    "- `travelTime`: estimated travel time for the segment (seconds).\n",
    "- `confidence` / `level`: a reliability indicator for the estimate.\n",
    "- `length` / `segmentLength`: segment length used to compute travel time / speed.\n",
    "\n",
    "TomTom computes traffic by comparing observed speeds to free-flow speeds, applying smoothing, filtering outliers, and using model-based fusion across probes and infrastructure to produce segment-level estimates and confidence scores. For prediction we typically use `currentSpeed` (or `travelTime`) as the target, and time-of-day, day-of-week, weather or nearby segment speeds as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de0dc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0286c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load historical JSON dumps from data/tomtom/<region>/\n",
    "def load_tomtom_jsons(region: str, data_root: Path = Path('..') / 'data' / 'tomtom') -> pd.DataFrame:\n",
    "    folder = data_root / region\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f'Folder not found: {folder}')\n",
    "    files = sorted(glob.glob(str(folder / '*.json')))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No JSON files found in {folder}')\n",
    "\n",
    "    rows = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            j = json.load(open(fp, 'r', encoding='utf-8'))\n",
    "        except Exception:\n",
    "            # skip broken files\n",
    "            continue\n",
    "        # expect a list of records or a top-level object with 'records'/'items'\n",
    "        if isinstance(j, dict):\n",
    "            # common patterns: {'records':[...]} or direct mapping\n",
    "            candidates = None\n",
    "            for k in ('records', 'items', 'features', 'results'):\n",
    "                if k in j and isinstance(j[k], list):\n",
    "                    candidates = j[k]\n",
    "                    break\n",
    "            if candidates is None:\n",
    "                # try to interpret dict as a single record\n",
    "                candidates = [j]\n",
    "        elif isinstance(j, list):\n",
    "            candidates = j\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for rec in candidates:\n",
    "            # normalize: many records will already be flat dicts\n",
    "            rows.append(rec)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.json_normalize(rows)\n",
    "\n",
    "    # Heuristic: try to find a timestamp column and convert to datetime\n",
    "    time_cols = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower()]\n",
    "    ts_col = None\n",
    "    for c in time_cols:\n",
    "        try:\n",
    "            s = pd.to_datetime(df[c], utc=True, errors='coerce')\n",
    "            if s.notna().sum() > 0:\n",
    "                ts_col = c\n",
    "                df[c] = s\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if ts_col is None:\n",
    "        # fallback: use file's ctime if available (won't be very accurate)\n",
    "        df['__file'] = df.get('__file', '')\n",
    "\n",
    "    # make sure common numeric fields exist (currentSpeed, freeFlowSpeed, travelTime)\n",
    "    for nm in ('currentSpeed','freeFlowSpeed','travelTime','speed'):\n",
    "        if nm in df.columns:\n",
    "            # coerce to numeric\n",
    "            df[nm] = pd.to_numeric(df[nm], errors='coerce')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9609cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['lat', 'lon', 'currentSpeed', 'freeFlowSpeed', 'confidence',\n",
       "       'currentTravelTime', 'freeFlowTravelTime', 'street', 'address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: load historical data for a region\n",
    "region = 'eindhoven'  # change to your region folder name (slugified)\n",
    "hist = load_tomtom_jsons(region)\n",
    "print('Loaded rows:', len(hist))\n",
    "hist.columns[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1a79bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering helper\n",
    "def prepare_features(df: pd.DataFrame, timestamp_col_candidates=None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # find timestamp column\n",
    "    ts_cols = timestamp_col_candidates or [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower() or c.lower()=='timestamp']\n",
    "    ts_col = None\n",
    "    for c in ts_cols:\n",
    "        if c in df.columns:\n",
    "            try:\n",
    "                df['_ts'] = pd.to_datetime(df[c], utc=True, errors='coerce')\n",
    "                if df['_ts'].notna().sum() > 0:\n",
    "                    ts_col = c\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "    if ts_col is None and '__file' in df.columns:\n",
    "        df['_ts'] = pd.NaT\n",
    "\n",
    "    # if still no ts, create a dummy increasing index time\n",
    "    if df.get('_ts').isna().all():\n",
    "        df['_ts'] = pd.date_range('2000-01-01', periods=len(df), freq='T', tz='UTC')\n",
    "\n",
    "    df['_hour'] = df['_ts'].dt.hour\n",
    "    df['_dow'] = df['_ts'].dt.dayofweek\n",
    "    # cyclic encoding for hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['_hour'] / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['_hour'] / 24.0)\n",
    "\n",
    "    # spatial grouping: round lat/lon if available to group nearby samples\n",
    "    lat_cols = [c for c in df.columns if c.lower().endswith('lat') or c.lower().startswith('lat')]\n",
    "    lon_cols = [c for c in df.columns if c.lower().endswith('lon') or c.lower().startswith('lon')]\n",
    "    if lat_cols and lon_cols:\n",
    "        latc = lat_cols[0]; lonc = lon_cols[0]\n",
    "        df['_plat'] = pd.to_numeric(df[latc], errors='coerce').round(5)\n",
    "        df['_plon'] = pd.to_numeric(df[lonc], errors='coerce').round(5)\n",
    "        df['_pkey'] = df['_plat'].astype(str) + '_' + df['_plon'].astype(str)\n",
    "    else:\n",
    "        df['_pkey'] = 'no_spatial'\n",
    "\n",
    "    # target: prefer currentSpeed, fallback to speed or travelTime/length if available\n",
    "    if 'currentSpeed' in df.columns:\n",
    "        df['target_speed'] = pd.to_numeric(df['currentSpeed'], errors='coerce')\n",
    "    elif 'speed' in df.columns:\n",
    "        df['target_speed'] = pd.to_numeric(df['speed'], errors='coerce')\n",
    "    elif 'travelTime' in df.columns and 'length' in df.columns:\n",
    "        # compute speed = length (m) / travelTime (s) -> km/h\n",
    "        df['target_speed'] = pd.to_numeric(df['length'], errors='coerce') / pd.to_numeric(df['travelTime'], errors='coerce') * 3.6\n",
    "    else:\n",
    "        df['target_speed'] = np.nan\n",
    "\n",
    "    # sort and create simple lag features per pkey\n",
    "    df = df.sort_values('_ts')\n",
    "    df['lag1'] = df.groupby('_pkey')['target_speed'].shift(1)\n",
    "    df['lag2'] = df.groupby('_pkey')['target_speed'].shift(2)\n",
    "    df['rolling_mean_3'] = df.groupby('_pkey')['target_speed'].rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    # drop rows without target\n",
    "    df = df[~df['target_speed'].isna()]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001c7af",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2684817235.py, line 178)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('Plot failed:', e)except Exception as e:    plt.show()    plt.xlabel('lon'); plt.ylabel('lat')    plt.title('Predicted speeds (spatial sample)')    plt.colorbar(sc, label='predicted speed (km/h)')    sc = plt.scatter(live_prep['_plon'], live_prep['_plat'], c=live_prep['pred_speed'], cmap='viridis', s=40)    plt.figure(figsize=(10,6))try:# Quick visualization: actual vs predicted scatter on map (lon/lat color-coded)\u001b[39m\n                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset and train an improved model (LightGBM/XGBoost fallback, time-aware split, randomized CV)\n",
    "df_feat = prepare_features(hist)\n",
    "print('After feature prep:', len(df_feat))\n",
    "# expand candidate features (lags and cyclical time)\n",
    "base_feats = ['hour_sin','hour_cos','_dow','lag1','lag2','rolling_mean_3']\n",
    "feat_cols = [c for c in base_feats if c in df_feat.columns]\n",
    "X = df_feat[feat_cols].fillna(-1)\n",
    "y = df_feat['target_speed']\n",
    "\n",
    "# Basic data checks and defensive behavior\n",
    "if df_feat.empty or len(X) == 0:\n",
    "    print('Not enough data after feature engineering to train a model.')\n",
    "else:\n",
    "    # Time-based split: train on earlier data, test on the most recent 2 days if possible\n",
    "    if '_ts' in df_feat.columns and df_feat['_ts'].notna().any():\n",
    "        cutoff = df_feat['_ts'].max() - pd.Timedelta(days=2)\n",
    "        train_idx = df_feat['_ts'] <= cutoff\n",
    "        test_idx = df_feat['_ts'] > cutoff\n",
    "        if train_idx.sum() < 100 or test_idx.sum() < 20:\n",
    "            # fallback to a random split if not enough data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "            y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print('Training rows:', len(X_train), 'Test rows:', len(X_test))\n",
    "\n",
    "    # If training set is small, skip expensive CV/hyper-search and use a simple estimator\n",
    "    MIN_SAMPLES_FOR_SEARCH = 50\n",
    "    model = None\n",
    "    if len(X_train) < MIN_SAMPLES_FOR_SEARCH:\n",
    "        print(f'Only {len(X_train)} training rows (<{MIN_SAMPLES_FOR_SEARCH}). Skipping hyperparameter search and using RandomForest.')\n",
    "        model = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        # Try LightGBM first (wrapped so any failure falls back cleanly)\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "            estimator = lgb.LGBMRegressor(objective='regression', n_jobs=-1, random_state=42)\n",
    "            # choose cv splits conservatively based on data size\n",
    "            n_splits = min(3, max(2, int(len(X_train) // 20)))\n",
    "            cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            param_dist = {\n",
    "                'n_estimators': [100, 200, 400],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'num_leaves': [31, 63, 127],\n",
    "                'max_depth': [-1, 6, 12],\n",
    "            }\n",
    "            rsearch = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=12, cv=cv, scoring='neg_mean_absolute_error', n_jobs=1, random_state=42, verbose=1)\n",
    "            rsearch.fit(X_train, y_train)\n",
    "            model = rsearch.best_estimator_\n",
    "            print('Best params (LightGBM):', rsearch.best_params_)\n",
    "        except Exception as e:\n",
    "            print('LightGBM not available or failed, fallback to XGBoost/RandomForest:', e)\n",
    "            try:\n",
    "                import xgboost as xgb\n",
    "                from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "                estimator = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=1, random_state=42)\n",
    "                n_splits = min(3, max(2, int(len(X_train) // 20)))\n",
    "                cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "                param_dist = {\n",
    "                    'n_estimators': [100,200,400],\n",
    "                    'learning_rate': [0.01,0.05,0.1],\n",
    "                    'max_depth': [3,6,10],\n",
    "                }\n",
    "                rsearch = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=12, cv=cv, scoring='neg_mean_absolute_error', n_jobs=1, random_state=42, verbose=1)\n",
    "                rsearch.fit(X_train, y_train)\n",
    "                model = rsearch.best_estimator_\n",
    "                print('Best params (XGBoost):', rsearch.best_params_)\n",
    "            except Exception as e2:\n",
    "                print('XGBoost not available or failed, using RandomForest:', e2)\n",
    "                model = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "    # If we used a search object, ensure model is fitted (some branches already fitted)\n",
    "    try:\n",
    "        model.predict(X_test[:1])\n",
    "    except Exception:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse_val = mean_squared_error(y_test, pred)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    print('MAE:', mae)\n",
    "    print('RMSE:', rmse_val)\n",
    "\n",
    "    # Feature importances if available\n",
    "    try:\n",
    "        importances = None\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "        elif hasattr(model, 'booster') and hasattr(model, 'get_score'):\n",
    "            # xgboost\n",
    "            try:\n",
    "                fmap = model.get_booster().get_score(importance_type='weight')\n",
    "                importances = pd.Series(fmap).reindex(X_train.columns).fillna(0).sort_values(ascending=False)\n",
    "            except Exception:\n",
    "                importances = None\n",
    "        if importances is not None:\n",
    "            print('Feature importances:')\n",
    "            print(importances)\n",
    "    except Exception as e:\n",
    "        print('Could not extract feature importances:', e)\n",
    "\n",
    "    # Save model and feature list\n",
    "    joblib.dump({'model': model, 'features': X_train.columns.tolist()}, f'model_{region}.joblib')\n",
    "    print('Saved model to', f'model_{region}.joblib')\n",
    "\n",
    "    # Diagnostic plots: predicted vs actual and error by hour\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.scatter(y_test, pred, alpha=0.4)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "        plt.xlabel('Actual speed')\n",
    "        plt.ylabel('Predicted speed')\n",
    "        plt.title('Actual vs Predicted')\n",
    "        plt.show()\n",
    "        # error by hour\n",
    "        res_df = pd.DataFrame({'actual': y_test, 'pred': pred})\n",
    "        res_df['_ts'] = df_feat.loc[y_test.index, '_ts'] if '_ts' in df_feat.columns else pd.NaT\n",
    "        if '_ts' in res_df and res_df['_ts'].notna().any():\n",
    "            res_df['_hour'] = res_df['_ts'].dt.hour\n",
    "            err_by_hour = (res_df['actual'] - res_df['pred']).abs().groupby(res_df['_hour']).mean()\n",
    "            plt.figure(figsize=(8,3))\n",
    "            err_by_hour.plot(kind='bar')\n",
    "            plt.title('Mean absolute error by hour')\n",
    "            plt.xlabel('Hour of day')\n",
    "            plt.ylabel('MAE (km/h)')\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('Plotting diagnostics failed:', e)\n",
    "\n",
    "\n",
    "# Use latest live JSON in folder as test dataset (if available)\n",
    "def load_latest_json_for_region(region: str, data_root: Path = Path('..') / 'data' / 'tomtom') -> Path:\n",
    "    folder = data_root / region\n",
    "    files = sorted(glob.glob(str(folder / '*.json')))\n",
    "    if not files:\n",
    "        return None\n",
    "    return Path(files[-1])\n",
    "\n",
    "latest = load_latest_json_for_region(region)\n",
    "if latest is not None:\n",
    "    print('Using latest file for live test:', latest)\n",
    "    live_df = load_tomtom_jsons(region)  # load all then filter by newest timestamps if needed\n",
    "    live_prep = prepare_features(live_df)\n",
    "    X_live = live_prep[ [c for c in feat_cols if c in live_prep.columns] ].fillna(-1)\n",
    "    preds = model.predict(X_live)\n",
    "    live_prep['pred_speed'] = preds\n",
    "    # evaluate where we have actuals\n",
    "    if 'target_speed' in live_prep.columns:\n",
    "        mae_live = mean_absolute_error(live_prep['target_speed'], live_prep['pred_speed'])\n",
    "        print('Live MAE:', mae_live)\n",
    "    display(live_prep[['target_speed','pred_speed','_ts','_plat','_plon']].head())\n",
    "# else:\n",
    "#     print('No live file found for region', region)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print('Plot failed:', e)except Exception as e:    plt.show()    plt.xlabel('lon'); plt.ylabel('lat')    plt.title('Predicted speeds (spatial sample)')    plt.colorbar(sc, label='predicted speed (km/h)')    sc = plt.scatter(live_prep['_plon'], live_prep['_plat'], c=live_prep['pred_speed'], cmap='viridis', s=40)    plt.figure(figsize=(10,6))try:# Quick visualization: actual vs predicted scatter on map (lon/lat color-coded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d016c4",
   "metadata": {},
   "source": [
    "- This notebook is a starter template. Improvements: spatial joins to canonical road segments, using TomTom segment IDs, adding weather and event features, and training more advanced time-series models (LSTM, temporal convolutional nets).\n",
    "- For production, collect stable segment identifiers (TomTom segment IDs) and build time-series per segment.\n",
    "- Consider persistent caching for reverse-geocode and careful rate-limit handling when fetching live data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
